---
layout: post
title: "集成学习 2025：XGBoost / LightGBM Stacking 与 Meta-Booster"
category: MachineLearning
date: 2025-07-01
tags:
  - 集成学习
  - XGBoost
  - LightGBM
  - Stacking
---

延续早年对简单 Ensemble 的讨论，集成学习在 2024–2025 年仍以 Stacking、Meta-Booster 等组合形式为重点。本文梳理近年实践与典型用法。

## 一、Stacking：多模型 + 元学习器

Stacking 将多个基学习器（如 XGBoost、LightGBM、CatBoost、AdaBoost）的输出作为特征，由**元模型**（如 Random Forest）做最终预测：

- **基学习器**：不同 boosting 算法，利用各自对表格数据的优势
- **元模型**：综合各基模型预测，提升泛化能力

在部分 MLPerf 2025 评测中，Stacking XGBoost 与 LightGBM 相比单模型可带来约 **10–15%** 的准确率提升，在表格数据上优势明显。

## 二、Meta-Booster：增量式多模型融合

**Meta-Booster** 在每个 boosting 步中融合多个基学习器的增量更新，支持：

- 基学习器：XGBoost、LightGBM、AdaBoost 以及神经网络
- 任务：分类与回归
- 特点：在每轮迭代中显式融合多模型更新，而非仅在最后 stacking

适合对精度和鲁棒性要求较高的表格任务。

## 三、算法特点简要对比

| 算法 | 特点 | 适用场景 |
|------|------|----------|
| XGBoost | 正则化强，剪枝有效 | 中小规模表格数据 |
| LightGBM | 直方图分桶，leaf-wise 生长 | 大规模数据，内存敏感 |
| CatBoost | 类别变量原生支持 | 高基数类别特征 |
| AdaBoost | 简单易用 | 基准与弱学习器组合 |

Stacking 和 Meta-Booster 都旨在利用这些算法的互补性。

## 四、工程实践要点

- **超参数**：Optuna、网格/随机搜索等工具广泛用于调参
- **数据增强**：与 SMOTE、噪声注入等结合，可提升稀疏或小样本表现
- **推理速度**：相比深度学习，集成模型在边缘设备上推理往往更快（约 30% 左右），适合部署

## 五、新兴方向：联邦学习与 IoT

集成方法与**联邦学习**结合，可在保护隐私的前提下联合多源数据训练；在 IoT 等边缘场景中，轻量级集成模型也有应用前景。

## 六、小结

Stacking 和 Meta-Booster 仍是 2025 年表格数据集成的主流思路，XGBoost 与 LightGBM 仍是基学习器首选。结合实际业务调参与数据增强，可获得稳定收益。
